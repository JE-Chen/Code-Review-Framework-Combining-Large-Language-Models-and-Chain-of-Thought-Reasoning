
Your task is to look at a given git diff that
represents a Python code change, linter
feedback and code smells detected in the code
change, and a corresponding review comment
about the diff. You need to rate how concise,
comprehensive, and relevant a review is and
whether it touches upon all the important
topics, code smells, vulnerabilities, and
issues in the code change.

Code Change:





Code Smells:
### Code Smell Type: Non-Deterministic Behavior Due to Uncontrolled Randomness
**Problem Location**:  
`load_data_but_not_really()`, `mysterious_transform()`, `aggregate_but_confusing()`, and `plot_something()`
```python
# Example from load_data_but_not_really()
value = np.random.randn(size) * random.choice([1, 10, 100])

# Example from mysterious_transform()
if random.random() > 0.5:
    df["value"] = df["value"].abs()

# Example from aggregate_but_confusing()
result.sort_values(by=random.choice(result.columns), ascending=random.choice([True, False]))
```

**Detailed Explanation**:  
This introduces **non-deterministic behavior** that violates core principles of data analysis. Results change unpredictably on each run due to:
- Unpredictable data generation (multiplier choice)
- Random transformations (value absolute)
- Non-deterministic sorting (column selection)
- Randomized plot labels  
This destroys reproducibility, breaks testing, and makes debugging impossible. Data analysis pipelines must be deterministic to ensure consistent results and validate findings.

**Improvement Suggestions**:  
1. Replace all `random` operations with explicit parameters:
   ```python
   def generate_synthetic_data(multiplier=1, seed=None):
       if seed is not None: np.random.seed(seed)
       return pd.DataFrame({
           "value": np.random.randn(size) * multiplier,
           ...
       })
   ```
2. Remove randomness from business logic:
   - Replace `random.choice` with configuration
   - Use stable sorting keys (e.g., `sort_values(by="value_mean")`)
3. Add seed management at pipeline entry point:
   ```python
   if __name__ == "__main__":
       np.random.seed(42)  # Fixed seed for reproducibility
       main()
   ```

**Priority Level**: High

---

### Code Smell Type: Non-Descriptive Naming
**Problem Location**:  
`load_data_but_not_really()`, `mysterious_transform()`, `aggregate_but_confusing()`, `df["category"] = df["category"].fillna("UNKNOWN")`
```python
# Function names
def load_data_but_not_really(): ...  # Misleading
def mysterious_transform(df): ...    # Vague

# Variable handling
df["category"] = df["category"].fillna("UNKNOWN")  # "UNKNOWN" is magic
```

**Detailed Explanation**:  
Names fail to communicate intent:
- `load_data_but_not_really` implies *intentional deception* rather than describing behavior
- `mysterious_transform` obscures logic (what transformation?)
- `UNKNOWN` is a magic string without context  
This creates cognitive load for maintainers and increases bug risk. Descriptive names enable self-documenting code.

**Improvement Suggestions**:  
1. Rename functions to reflect purpose:
   ```python
   # Instead of:
   def load_data_but_not_really():
   
   # Use:
   def generate_synthetic_data(size=30, multiplier=1):
   ```
2. Replace magic strings with constants:
   ```python
   UNKNOWN_CATEGORY = "UNKNOWN"  # At module level
   df["category"] = df["category"].fillna(UNKNOWN_CATEGORY)
   ```
3. Add docstrings explaining *why*:
   ```python
   def generate_synthetic_data(size=30, multiplier=1):
       """Generate test data with controlled variance.
       
       Args:
           size: Number of samples
           multiplier: Scale factor for value distribution
       """
   ```

**Priority Level**: High

---

### Code Smell Type: Violation of Single Responsibility Principle (SRP)
**Problem Location**:  
`mysterious_transform()` and `aggregate_but_confusing()`
```python
# Violates SRP: transforms AND filters
def mysterious_transform(df):
    df["value_squared"] = df["value"] ** 2
    if random.random() > 0.5:
        df["value"] = df["value"].abs()
    df = df[df["value"] > df["value"].mean() / 3]  # Filter
    return df

# Violates SRP: aggregates AND sorts
def aggregate_but_confusing(df):
    result = df.groupby(...).agg(...)
    result.columns = ...  # Rename
    return result.sort_values(...)  # Sort
```

**Detailed Explanation**:  
Each function performs multiple unrelated operations:
- `mysterious_transform` combines transformation, filtering, and side effects
- `aggregate_but_confusing` mixes aggregation with sorting logic  
This makes functions:
- Hard to test (multiple concerns)
- Impossible to reuse (e.g., can't aggregate without sorting)
- Prone to bugs when one concern changes

**Improvement Suggestions**:  
Split into focused functions:
```python
def transform_values(df):
    df = df.copy()
    df["value_squared"] = df["value"] ** 2
    df["value"] = df["value"].abs()
    return df

def filter_values(df, threshold=1/3):
    return df[df["value"] > df["value"].mean() * threshold]

def compute_aggregated_stats(df):
    return (
        df.groupby("category")
        .agg({"value": ["mean", "sum"], "flag": "count"})
        .rename(columns=lambda x: "_".join(x))
    )

def sort_aggregated_data(df, sort_key="value_mean", ascending=True):
    return df.sort_values(by=sort_key, ascending=ascending)
```

**Priority Level**: High

---

### Code Smell Type: Hardcoded Magic Values
**Problem Location**:  
`df = df[df["value"] > df["value"].mean() / 3]` and `random.choice([1, 10, 100])`
```python
# Magic number 3
df = df[df["value"] > df["value"].mean() / 3]

# Magic multiplier choices
value = np.random.randn(size) * random.choice([1, 10, 100])
```

**Detailed Explanation**:  
Arbitrary values (`3`, `[1,10,100]`) lack context:
- Why `/3`? Is this a statistical threshold?
- Why these specific multipliers?  
Magic numbers obscure business intent and make future changes error-prone.

**Improvement Suggestions**:  
1. Define constants with purpose:
   ```python
   VALUE_FILTER_THRESHOLD = 1/3  # 33% of mean
   MULTIPLIER_OPTIONS = [1, 10, 100]  # Configurable scales
   ```
2. Parameterize where possible:
   ```python
   def generate_synthetic_data(size, multiplier=1):
       return np.random.randn(size) * multiplier
   ```

**Priority Level**: Medium

---

### Code Smell Type: Inconsistent Data Handling
**Problem Location**:  
`category` is filled with `"UNKNOWN"` but `flag` is not handled
```python
df["category"] = df["category"].fillna("UNKNOWN")  # Handled
df["flag"] = np.random.choice([0, 1, None], size=size)  # Unhandled None
```

**Detailed Explanation**:  
`category` gets cleaned but `flag` retains `None` values. This inconsistency:
- Creates silent failures in aggregation (e.g., `flag.count()` ignores `None`)
- Requires mental context to remember which columns are safe
- Breaks the principle of uniform data treatment

**Improvement Suggestions**:  
Standardize missing value handling:
```python
def generate_synthetic_data(...):
    data = {
        "value": ...,
        "category": [random.choice(["A", "B", "C", None]) for _ in range(size)],
        "flag": np.random.choice([0, 1, None], size=size),
    }
    df = pd.DataFrame(data)
    # Handle missing values uniformly
    df["category"] = df["category"].fillna("UNKNOWN")
    df["flag"] = df["flag"].fillna(0)  # Explicit default
    return df
```

**Priority Level**: Medium

---

### Summary of Recommendations
| Priority | Smell Type                          | Impact                                                                 |
|----------|-------------------------------------|------------------------------------------------------------------------|
| High     | Non-deterministic behavior          | Breaks reproducibility, testing, and debugging                         |
| High     | Non-descriptive naming              | Increases cognitive load and bug risk                                  |
| High     | SRP violation                       | Blocks maintainability and testability                                 |
| Medium   | Magic numbers                       | Hides business intent and increases change risk                        |
| Medium   | Inconsistent data handling          | Creates silent failures and maintenance surprises                      |

**Critical Path**: Fix non-determinism first (High priority) to enable reproducibility. Then address naming and SRP violations. This transforms the script from a fragile prototype into a maintainable analysis tool.


Linter Messages:
[
  {
    "rule_id": "bad-function-name",
    "severity": "warning",
    "message": "Function name 'load_data_but_not_really' is misleading as it generates random data instead of loading from a file.",
    "line": 12,
    "suggestion": "Rename to 'generate_random_data' or similar to accurately describe the function's purpose."
  },
  {
    "rule_id": "bad-function-name",
    "severity": "warning",
    "message": "Function name 'mysterious_transform' is non-descriptive and fails to convey the transformation logic.",
    "line": 26,
    "suggestion": "Rename to a descriptive name such as 'filter_and_square_values'."
  },
  {
    "rule_id": "bad-function-name",
    "severity": "warning",
    "message": "Function name 'aggregate_but_confusing' is misleading and does not reflect the non-deterministic sorting.",
    "line": 37,
    "suggestion": "Rename to 'aggregate_and_sort' and remove randomness or make it configurable."
  },
  {
    "rule_id": "missing-docstring",
    "severity": "warning",
    "message": "Function is missing a docstring to explain its purpose, parameters, and return values.",
    "line": 12,
    "suggestion": "Add a docstring describing the function's behavior and inputs."
  },
  {
    "rule_id": "non-deterministic-behavior",
    "severity": "warning",
    "message": "Function 'mysterious_transform' contains non-deterministic behavior via random choice in condition.",
    "line": 29,
    "suggestion": "Make the behavior deterministic by removing randomness or using a fixed seed."
  },
  {
    "rule_id": "non-deterministic-behavior",
    "severity": "warning",
    "message": "Function 'aggregate_but_confusing' uses random column and order for sorting, making output non-reproducible.",
    "line": 49,
    "suggestion": "Use a deterministic sort key or allow the sort order to be specified as a parameter."
  }
]


Review Comment:
First code review: 

Here's a concise code review focusing on the most obvious issues:

- **Naming Conventions**:  
  Functions like `load_data_but_not_really` and `mysterious_transform` are misleading and unprofessional. Rename to `generate_sample_data` and `clean_and_transform_values` for clarity.

- **Non-Determinism**:  
  Random operations (`random.choice`, `random.random()`) in `mysterious_transform` and `aggregate_but_confusing` break reproducibility. Remove randomness for testability and consistency.

- **Logic Ambiguity**:  
  In `mysterious_transform`, the condition `df["value"] > df["value"].mean() / 3` may unintentionally filter data due to negative means. Add explicit handling for edge cases.

- **Missing Documentation**:  
  No docstrings for functions. Add brief descriptions of purpose, inputs, and outputs (e.g., "`generate_sample_data` creates synthetic DataFrame with randomized values").

- **Redundant Checks**:  
  The `if len(df) > 0` in `main()` is unnecessary since `mysterious_transform` filters rows and returns empty DFs gracefully.

- **Inconsistent Naming**:  
  Use `value` consistently instead of mixing `value` and `value_squared` in plots. Rename `plot_something` to `plot_value_relationship` for specificity.

- **Resource Handling**:  
  `plt.show()` blocks execution. Add `plt.close()` after plot to prevent resource leaks in repeated runs.

First summary: 

### Code Review: `analysis.py`

#### âœ… **Readability & Consistency**
- **Good**: Indentation is consistent (4 spaces), and the structure is clean.
- **Issue**: Non-descriptive function names (`load_data_but_not_really`, `mysterious_transform`) obscure intent.  
  *Recommendation*: Replace with meaningful names (e.g., `generate_synthetic_data`).

#### âš ï¸ **Naming Conventions**
- **Critical**: Names like `mysterious_transform` and `aggregate_but_confusing` are unprofessional and reduce clarity.  
  *Example Fix*:  
  `mysterious_transform` â†’ `filter_and_square_values`  
  `aggregate_but_confusing` â†’ `aggregate_by_category`
- **Missing**: No docstrings for functions or key variables.

#### âš ï¸ **Software Engineering Standards**
- **Modularity**: Functions are tightly coupled (e.g., `main` handles data flow, transformation, and visualization).  
  *Recommendation*: Split into independent components (e.g., separate data generation, transformation, aggregation, and plotting).
- **Testability**: Non-determinism (via `random`) breaks testing.  
  *Example*: `mysterious_transform` randomly applies `.abs()` and filters.

#### âš ï¸ **Logic & Correctness**
- **Critical Risk**:  
  - `mysterious_transform` uses random filtering (`df["value"] > df["value"].mean() / 3`), causing inconsistent results.  
  - `aggregate_but_confusing` randomly selects sort columns (`random.choice(result.columns)`), making outputs unpredictable.
- **Edge Case**: `load_data_but_not_really` allows `None` in `category`/`flag`, but later fills `category` with `"UNKNOWN"`â€”this is inconsistent.
- **Safety**: No input validation (e.g., `df` could be empty before `.mean()`).

#### ðŸ›¡ï¸ **Performance & Security**
- **Low Impact**: No obvious bottlenecks or security risks.  
- **Note**: Randomness in critical paths (e.g., filtering) is a design flaw, not a performance issue.

#### ðŸ“š **Documentation & Testing**
- **Missing**: No docstrings or inline comments explaining *why* logic exists.
- **Untestable**: Randomness prevents deterministic unit tests.

---

### ðŸ”§ **Key Fixes Required**
| Issue                          | Before                          | After                              |
|--------------------------------|---------------------------------|------------------------------------|
| **Non-determinism**            | `random.choice` in core logic   | Replace with fixed parameters/config |
| **Poor naming**                | `mysterious_transform`          | `filter_and_square_values`         |
| **Missing validation**         | No checks for empty DataFrames  | Add `if df.empty: return df`       |
| **Inconsistent handling**      | `None` in `category` + fill     | Standardize input validation       |

---

### ðŸ’¡ **Recommendations for PR**
1. **Remove randomness** from business logic (e.g., make filter thresholds configurable).
2. **Rename all functions** to reflect purpose (avoid sarcasm).
3. **Add docstrings** explaining:
   - Input/output expectations.
   - Rationale for key decisions (e.g., "Filter values > 1/3 of mean").
4. **Split `main()`** into separate testable functions.
5. **Add unit tests** for deterministic paths (e.g., test aggregation output).

---

### âš ï¸ **Risks to Address**
- **Unpredictable outputs**: Random sorting/filtering could break downstream consumers.
- **Debugging difficulty**: Non-determinism hides root causes.
- **Testing gaps**: Without deterministic behavior, coverage is impossible.

---

### âœ… **Items for Reviewers to Confirm**
1. Are the new function names (e.g., `filter_and_square_values`) clear and actionable?
2. Does the fix for randomness (e.g., replacing `random.choice` with config) align with team standards?
3. Is input validation added for empty DataFrames?
4. Are there unit tests covering edge cases (e.g., empty `df`, negative values)?

> **Note**: This code is currently **unusable in production** due to non-determinism. Prioritize fixing randomness and naming before merging.

Total summary: 

 ### PR Total Summary

1. **Overall conclusion**  
   - **Fails to meet merge criteria** due to critical blocking issues.  
   - **Blocking concerns**: Non-deterministic behavior (High Priority) and non-descriptive naming (High Priority) make the code unusable in production.  
   - **Non-blocking concerns**: Magic values (Medium Priority) and inconsistent data handling (Medium Priority) require attention but are secondary to blocking issues.

2. **Comprehensive evaluation**  
   - **Code quality & correctness**:  
     Non-determinism (e.g., `random.choice` in `load_data_but_not_really`, `mysterious_transform`, and `aggregate_but_confusing`) causes unpredictable outputs and breaks reproducibility. The condition `df["value"] > df["value"].mean() / 3` risks filtering data incorrectly when means are negative. *Evidence: Code Smell Result (Non-Deterministic Behavior), Linter (non-deterministic-behavior), First Summary Result (Critical Risk)*.  
   - **Maintainability & design**:  
     Functions violate SRP (e.g., `mysterious_transform` combines transformation, filtering, and side effects) and lack meaningful names (`mysterious_transform`), increasing cognitive load. *Evidence: Code Smell Result (SRP Violation, Non-Descriptive Naming), First Summary Result (Modularity & Naming)*.  
   - **Consistency with standards**:  
     Violates naming conventions (e.g., `load_data_but_not_really` implies file loading, but generates synthetic data) and lacks docstrings. *Evidence: Linter (bad-function-name, missing-docstring), First Code Review Result (Naming Conventions)*.

3. **Final decision recommendation**  
   - **Request changes**.  
   - **Justification**: The code is currently **unusable in production** due to non-determinism (e.g., random filtering/sorting) and misleading names. These issues prevent reliable testing, debugging, and deployment. *Evidence: First Summary Result ("This code is currently unusable in production"), Code Smell Result (Non-Deterministic Behavior Priority: High)*.

4. **Team follow-up**  
   - **Immediate actions**:  
     1. Replace all `random` operations with configurable parameters (e.g., `multiplier` in `load_data_but_not_really`).  
     2. Rename all functions to reflect purpose (e.g., `mysterious_transform` â†’ `filter_and_square_values`).  
     3. Add docstrings explaining business intent (e.g., "Filters values > 1/3 of mean").  
   - **Validation**: Confirm fixes resolve non-determinism and naming before merging. *Evidence: Code Smell Result (Improvement Suggestions), First Summary Result (Key Fixes Required)*.

Step by step analysis: 

### Code Quality Review Report

---

#### 1. **Misleading Function Name: `load_data_but_not_really`**  
*(Linter: `bad-function-name`, Line 12)*  
- **Issue**: Function generates random data instead of loading from a file.  
- **Root Cause**: Names imply *intentional deception* ("not really") rather than describing actual behavior.  
- **Impact**: Breaks reader expectations, increases debugging time, and violates the principle of self-documenting code.  
- **Fix**:  
  ```python
  # BEFORE
  def load_data_but_not_really(size=30):
      return pd.DataFrame({"value": np.random.randn(size) * random.choice([1, 10, 100])})
  
  # AFTER
  def generate_synthetic_data(size=30, multiplier=1):
      """Generate random test data with configurable scaling.
      
      Args:
          size: Number of samples.
          multiplier: Scale factor for value distribution.
      """
      return pd.DataFrame({"value": np.random.randn(size) * multiplier})
  ```
- **Best Practice**: Use *behavior-focused names* (e.g., `generate_*`, `create_*`). Avoid negative phrasing ("not really").

---

#### 2. **Non-Descriptive Function Name: `mysterious_transform`**  
*(Linter: `bad-function-name`, Line 26)*  
- **Issue**: Name fails to convey transformation logic (squaring values + filtering).  
- **Root Cause**: Lazy naming instead of describing *what* the function does.  
- **Impact**: Forces readers to inspect implementation to understand purpose.  
- **Fix**:  
  ```python
  # BEFORE
  def mysterious_transform(df):
      df["value_squared"] = df["value"] ** 2
      if random.random() > 0.5:  # Non-deterministic!
          df["value"] = df["value"].abs()
      return df[df["value"] > df["value"].mean() / 3]
  
  # AFTER (with deterministic fix)
  def filter_and_square_values(df, threshold=1/3):
      """Square values and filter below threshold.
      
      Args:
          df: DataFrame with 'value' column.
          threshold: Fraction of mean to filter below.
      """
      df = df.copy()
      df["value_squared"] = df["value"] ** 2
      return df[df["value"] > df["value"].mean() * threshold]
  ```
- **Best Practice**: Name functions after *outcomes* (e.g., `filter_*`, `transform_*`), not vague metaphors.

---

#### 3. **Non-Deterministic Behavior in `mysterious_transform`**  
*(Linter: `non-deterministic-behavior`, Line 29)*  
- **Issue**: Random `if` condition breaks reproducibility.  
- **Root Cause**: Uncontrolled randomness in business logic.  
- **Impact**: Tests fail unpredictably; results cannot be validated.  
- **Fix**:  
  - Remove randomness or parameterize it:  
    ```python
    # REMOVE RANDOMNESS (preferred for analysis)
    def filter_and_square_values(df, threshold=1/3, use_abs=False):
        df = df.copy()
        df["value_squared"] = df["value"] ** 2
        if use_abs:
            df["value"] = df["value"].abs()
        return df[df["value"] > df["value"].mean() * threshold]
    
    # OR (if randomness is intentional for testing)
    def filter_and_square_values(df, threshold=1/3, seed=None):
        if seed is not None:
            np.random.seed(seed)  # Ensure reproducibility
        # ... rest of logic
    ```
- **Best Practice**: *Data pipelines must be deterministic*. Use seeds or remove randomness from core logic.

---

#### 4. **Misleading Function Name: `aggregate_but_confusing`**  
*(Linter: `bad-function-name`, Line 37)*  
- **Issue**: Name doesnâ€™t reflect non-deterministic sorting.  
- **Root Cause**: Ignored behavioral details (random column/order) in naming.  
- **Impact**: Users assume consistent sorting, but output varies.  
- **Fix**:  
  ```python
  # BEFORE
  def aggregate_but_confusing(df):
      result = df.groupby(...).agg(...)
      result.sort_values(by=random.choice(result.columns), ascending=random.choice([True, False]))
      return result
  
  # AFTER (deterministic)
  def aggregate_and_sort(df, sort_key="value_mean", ascending=True):
      """Aggregate and sort by specified key.
      
      Args:
          df: Input DataFrame.
          sort_key: Column to sort by (e.g., 'value_mean').
          ascending: Sort order.
      """
      result = df.groupby("category").agg({"value": ["mean", "sum"]})
      result.columns = [f"{col[0]}_{col[1]}" for col in result.columns]
      return result.sort_values(by=sort_key, ascending=ascending)
  ```
- **Best Practice**: *Names must match behavior*. If randomness is unavoidable, document it explicitly (e.g., `aggregate_and_sort_randomly`).

---

#### 5. **Non-Deterministic Sorting in `aggregate_but_confusing`**  
*(Linter: `non-deterministic-behavior`, Line 49)*  
- **Issue**: Random column/order selection makes output unpredictable.  
- **Root Cause**: Used `random.choice` where a stable sort key was expected.  
- **Impact**: Critical for reproducibility; breaks all downstream analysis.  
- **Fix**: Parameterize sort key (as shown in #4).  
- **Best Practice**: *Never use randomness in data processing logic*. If sorting order must be configurable, expose it as a parameter.

---

#### 6. **Missing Docstring**  
*(Linter: `missing-docstring`, Line 12)*  
- **Issue**: No documentation for `load_data_but_not_really`.  
- **Root Cause**: Overlooked documentation as "low priority".  
- **Impact**: Prevents understanding of parameters/return values.  
- **Fix**: Add concise docstring (as shown in #1).  
- **Best Practice**: *Document every function*. A good docstring answers: *What does it do? Inputs? Outputs?*

---

### Summary of Critical Fixes
| Issue Type                     | Priority | Why Fix First?                                                                 |
|--------------------------------|----------|-------------------------------------------------------------------------------|
| Non-deterministic behavior     | High     | Breaks reproducibilityâ€”*core requirement for data analysis*.                   |
| Non-descriptive naming         | High     | Causes immediate confusion and errors.                                         |
| Missing docstrings             | Medium   | Slows onboarding but less critical than non-determinism.                        |
| Magic values (e.g., `/3`)      | Medium   | Hinders understanding of business logic.                                       |

> **Key Insight**: Non-determinism is the *highest-risk issue*. Fixing randomness first enables testing, validation, and trust in the pipeline. Naming and documentation are foundational for maintainability but secondary to reproducibility.  

**Final Recommendation**:  
> Prioritize removing all randomness from business logic (use fixed seeds or deterministic parameters), then rename functions to reflect true behavior. Document everything. This transforms the code from a fragile prototype into a reliable tool.


You should first generate a step-by-step list
of all the topics the review should cover like
code smells, issues that would be flagged by a
linter, security vulnerabilities, etc. Also,
the review should cover aspects like bugs, code
security, code readability, maintainability,
memory consumption, performance, good and bad
design patterns, and efficiency introduced in
the code change. Put your analysis under a
section titled \### Topics to be Covered:".

After generating the list above you should
again think step-by-step about the given review
comment and whether it addresses these topics
and put it under a section called "###
Step-by-Step Analysis of Review Comment:". Then
based on your step-by-step analysis you should
generate a score ranging from 1 (minimum value)
to 5 (maximum value) each about how
comprehensive, concise, and relevant a review
is. A review getting a score of 5 on
comprehensiveness addresses nearly all the
points in the \### Topics to be Covered:"
section while a review scoring 1 addresses none
of them. A review getting a score of 5 on
conciseness only covers the topics in the \###
Topics to be Covered:" section without wasting
time on off-topic information while a review
getting a score of 1 is entirely off-topic.
Finally, a review scoring 5 on relevance is
both concise and comprehensive while a review
scoring 1 is neither concise nor comprehensive,
effectively making relevance a combined score
of conciseness and comprehensiveness. You
should give your final rating in a section
titled \### Final Scores:". give the final scores as shown
below (please follow the exact format).

### Final Scores:
```
("comprehensiveness": your score, "conciseness": your score,
"relevance": your score)
```
Now start your analysis starting with the \###
Topics to be Covered:", followed by "###
Step-by-Step Analysis of Review Comment:" and
ending with the \### Final Scores:".

### Topics to be Covered:
(topics_to_be_covered)
